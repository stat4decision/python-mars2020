{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Un premier regard sur un réseau de neurones\n",
    "\n",
    "- Premier exemple concret de réseau de neurones, qui utilise le package Python Keras pour apprendre à classer chiffres écrits à la main. \n",
    "\n",
    "\n",
    "- Le problème que nous essayons de résoudre ici est de classer les images en niveaux de gris de chiffres manuscrits (28 pixels sur 28 pixels) dans leur 10 catégories (0 à 9). \n",
    "\n",
    "\n",
    "- Le jeu de données que nous allons utiliser est le MNIST, un jeu de données classique de la communauté d’apprentissage automatique, qui a été presque aussi longtemps que le champ lui-même et a été très intensément étudié. C'est un ensemble de 60 000 images d'apprentissage, plus 10 000 de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Etudions les données d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(train_images[33])\n",
    "print(train_labels[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notre flux de travail sera le suivant: d’abord, nous présenterons à notre réseau de neurones les données d’entraînement, `train_images` et` train_labels`.\n",
    "\n",
    "\n",
    "Le réseau apprendra ensuite à associer des images et des étiquettes. Enfin, nous demanderons au réseau de produire des prédictions pour `test_images`, et nous vérifierons si ces prédictions correspondent aux étiquettes de `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s4d-asus-14\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ici, notre réseau consiste en une séquence de deux couches `Dense`, qui sont des couches densément connectées.\n",
    "\n",
    "\n",
    "La deuxième (et dernière) couche est une couche \"softmax\", ce qui signifie qu’elle renverra un tableau de 10 scores de probabilité\n",
    "\n",
    "\n",
    "Nous devons choisir trois éléments supplémentaires dans le cadre de l’étape \"compilation\":\n",
    "\n",
    "- Une fonction de perte: voici comment le réseau sera capable de mesurer la qualité de son travail sur ses données de formation, et donc comment il le sera capable de se diriger dans la bonne direction.\n",
    "-  Un optimiseur: c'est le mécanisme par lequel le réseau se mettra à jour en fonction des données qu'il voit et de sa fonction de perte.\n",
    "- Mesures : pendant la formation et les tests. Ici, nous ne nous intéresserons qu'à la précision (la fraction des images qui ont été correctement classifié)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Avant l'apprentissage, nous allons prétraiter nos données en les transformant dans la forme attendue par le réseau et en les redimensionnant de manière à ce que toutes les valeurs soient en mémoire.\n",
    "\n",
    "\n",
    "Nos images étaient stockées dans un tableau de formes `(60000, 28, 28)` de type `uint8` avec des valeurs dans l'intervalle `[0, 255]`. Nous le transformons en un `tableau float32` de forme` (60000, 28 * 28) `avec des valeurs comprises entre 0 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Nous devons également coder les étiquettes de manière catégorielle : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nous sommes maintenant prêts à apprendre notre réseau via un appel à la méthode `fit` du réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2758 - acc: 0.8991 - val_loss: 0.3355 - val_acc: 0.8814\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2615 - acc: 0.9038 - val_loss: 0.3909 - val_acc: 0.8633\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2512 - acc: 0.9068 - val_loss: 0.3580 - val_acc: 0.8811\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2409 - acc: 0.9102 - val_loss: 0.3873 - val_acc: 0.8707\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2334 - acc: 0.9129 - val_loss: 0.3435 - val_acc: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bad9baef60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128,validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Deux quantités sont affichées pendant l’apprentissage : la \"perte\" du réseau sur les données d’apprentissage et la précision du réseau sur les données d'entraînement.\n",
    "\n",
    "Nous atteignons rapidement une précision de 0,989 (soit 98,9%) sur les données d’apprentissage. Vérifions maintenant que notre modèle fonctionne bien sur le jeu de tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.8663\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
